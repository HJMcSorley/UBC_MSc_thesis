---
title: "Reproducible data analysis: thesis data wrangling"
subtitle: "Pacific Maritime forWater Masters Project (NSERC forWater)"
author: "Hannah J McSorley"
output: bookdown::word_document2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, package.startup.message = FALSE, fig.path="R-outputs_UBC-forWater-MSc_HMc/figures/")
```

# Setup

note that rainmaker package is only available on GitHub (under development)
```
install.packages("devtools")
devtools::install_github("USGS-R/Rainmaker")
```

```{r wrangling packages, include = FALSE}

# load packages
library(tidyverse)  # tidyverse includes: dplyr, ggplot2, purrr, readr, forcats
library(knitr)      # tidy tables, knitting docs
library(lubridate)  # dates, times, ranges
library(broom)      # tidy stats
library(Rainmaker)  # USGS tool -- https://github.com/USGS-R/Rainmaker

```

assign time zone and create functions

```{r, function}
# time-zone note: all loggers record in standard time (no daylight savings shift)
TZ <- "Etc/GMT+8"

# create a function:
# ---- Alpha-numeric extraction function ---- #
# from http://stla.github.io/stlapblog/posts/Numextract.html
NumberXtract <- function(alphnum){
  unlist(regmatches(alphnum, gregexpr("[[:digit:]]+\\.*[[:digit:]]*", alphnum)))
}

```

# Tracking field activity

Multiple trips were taken to the field (Leech River watershed) and the trip number and/or date were used to organize files for sample identification, analytical results, joining dataframes and matching sample data to field-based data logger intervals.

This code brings in data that documents installation locations field trip tracking.

```{r, tracking}

# read in file used to track trips
trip_df <- read.csv(file = "R-inputs_UBC-forWater-MSc_HMc/Leech-FieldTrip-tracking_forWater-MSc_HMc.csv", 
                    header = TRUE)
str(trip_df)

# adjust format of dates
trip_df <- trip_df %>%
  mutate(trip = as_factor(trip),
         trip.start = lubridate::ymd(trip.start, tz = TZ),
         trip.end = lubridate::ymd(trip.end, tz = TZ),
         analysis_date_shimadzu = lubridate::ymd(analysis_date_shimadzu, tz = TZ),
         analysis_date_scan = lubridate::ymd(analysis_date_scan, tz = TZ),
         note = as.character(note))
str(trip_df)

# site installation locations
install_df <- read.csv(file = "R-inputs_UBC-forWater-MSc_HMc/Leech-installation-locations_forWater-MSc_HMc.csv",
                       header = TRUE) %>%
  mutate(Site_Number = as_factor(Site_Number))
```

# Water sample data and analytic results

Water samples were collected and transported via coolers (on ice) to UBC's EcoHydrology Lab for analysis of dissolved organic carbon (DOC) concentrations and indicators of NOM character. 

For quantification of DOC, samples were filtered with 0.45-micron PES filters; acidified to bring pH below 2; and analyzed for non-purgeable organic carbon (NPOC) via High-Temperature Combustion Method (5310-B) on a Shimadzu TOC-V.

Spectral properties of sample NOM were analyzed using a "Spectro::lyser" spectrophotometer (S::can, Vienna, Austria) which measures turbidity and the chromophoric portion of dissolved organic matter to estimate concentrations of total organic carbon (TOC), DOC as well as nitrate-nitrogen (NO^-3^-N). 

## DOC: Shimadzu TOC-V for DOC concentrations (as mg/L NPOC)

Water samples were filtered (0.45-micron PES filters) and acidified to bring pH below 2, then sparged with ultra-pure hydrocarbon-free air to drive off inorganic carbon. Following sparging, the sample was combusted to convert all organic carbon to carbon dioxide which was measured with a non-dispersive infrared gas detector to quanitify the DOC content of the sample. This method representd the most direct method of measuring DOC, as all natural organic carbon in the sample is measured. However, very small volatile organic carbon compounds would be removed in the sparging process. Because most NOM compounds are of higher molecular weight, it is unlikely that NOM DOC analytes would be removed.

```{r, TOC-V}
## Shimadzu TOC-V samples and results
# sample IDs and results from analysis of DOC (as NPOC)
# three part data munge
# 1. bring in sample tracking data (vial number, name, etc), format appropriately
# 2. bring in results data (from instrument), format appropriately
# 3. bring them together as meanigful results data (sample IDs + results)


# 1. ------------ SAMPLES ------------ #

# create a data frame of all sample ID data
# for all samples analyzed on Shimadzu TOC-V (UBC ESB 3062)


# read in all the files
shimadzu_samples <- 
  list.files(path = "R-inputs_UBC-forWater-MSc_HMc/shimadzu/samples_TOC/", 
             pattern = "*.csv") %>% 
  purrr::map(~ read_csv(file.path("R-inputs_UBC-forWater-MSc_HMc/shimadzu/samples_TOC/", .), 
                        skip = 4, col_names = TRUE)) %>% 
  reduce(rbind)

# check structure
# str(shimadzu_samples)

# correct the structure/format
shimadzu_samples <- shimadzu_samples %>% 
  dplyr::transmute(vial = as.numeric(vial),
                   site = forcats::as_factor(site),
                   sample_type = forcats::as_factor(`sample-type`),
                   sample = forcats::as_factor(sample),
                   dt_sampled = head(lubridate::ymd_hms(`date-time_sampled`, frac = TRUE, tz = TZ, truncated = 3), -1),
                   fillStage_cm = as.numeric(`fill-stage`),
                   analysis = as_factor(analysis),
                   trip = as_factor(trip))

# str(shimadzu_samples)
# head(shimadzu_samples)

#check number of trips (see 'trip_df')
# levels(shimadzu_samples$trip)

# check which sites are included
# levels(shimadzu_samples$site)

# fix naming errors for sites
shimadzu_samples$site <- shimadzu_samples$site %>% 
  plyr::revalue(c(
    "Chris" = "Chris-crk",
    "Lower-Leech-blw-confl" = "Leech-downstreamconf",
    "Leech-dwnstrm-confl" = "Leech-downstreamconf",
    "Leech-main-confl" = "Leech-downstreamconf",
    "Weeks-Lk-NE" = "Weeks-Lake",
    "J-Trib" = "West-Jordan",
    "West-jordan" = "West-Jordan",
    "W.Jarvis" = "Jarvis",
    "Rithet-N" = "Rithet",
    "Judge" = "Judge-crk"
  ))
# check sites again
# levels(shimadzu_samples$site)


# 2. ------------ RESULTS ------------ # 

# Note: annoying TOC-V export issue
# data files for analysis of trips 1-8 have 19 columns, while trips 9-23 have 18 columns
# import as two separate blocks (from two subdirectories), then alter, then merge

# assign column names = col.names():
nineteencolumns <- c("Type",	"Anal.",	"Sample Name",	"Sample ID",	"Origin",	"Cal. Curve",	"Manual Dilution	Notes",	"Date", "Time", "AM/PM",	"Spl. No.", "Inj. No.", "Analysis(Inj.)",	"Area",	"Mean Area",	"Conc.",	"Result",	"Excluded",	"Inj. Vol.")

# no "mean area" variable
eighteencolumns <- c("Type",	"Anal.",	"Sample Name",	"Sample ID",	"Origin",	"Cal. Curve",	"Manual Dilution	Notes",	"Date", "Time", "AM/PM",	"Spl. No.", "Inj. No.", "Analysis(Inj.)",	"Area",	"Conc.",	"Result",	"Excluded",	"Inj. Vol.")

## read in files and add a column indicating trip #, based on source file name

## directory 1 (trips 2-8) with 19 columns
# read in all the files (trips 2-8)
shimadzu_1 <- 
  list.files(path = "R-inputs_UBC-forWater-MSc_HMc/shimadzu/results_TOC/block1_19columns/", 
             pattern = "*.txt") %>% 
  set_names(str_extract(., "([a-z]{4,}+[0-9]*+)")) %>%
  purrr::map_dfr(~ read.table(file = file.path("R-inputs_UBC-forWater-MSc_HMc/shimadzu/results_TOC/block1_19columns/", .), 
                              row.names = NULL, skip = 14, header = FALSE, col.names = nineteencolumns), .id = "source") %>% 
  select(-Mean.Area)
# check that there are 19 columns now (minus "Mean.Area")
# str(shimadzu_1)


## directory 2 (trips 9-23) with 18 columns
# read in all the files (trips 9 -22)
shimadzu_2 <- 
  list.files(path = "R-inputs_UBC-forWater-MSc_HMc/shimadzu/results_TOC/block2_18columns/", 
             pattern = "*.txt") %>% 
  set_names(str_extract(., "([a-z]{4,}+[0-9]*+)")) %>%
  purrr::map_dfr(~ read.table(file = file.path("R-inputs_UBC-forWater-MSc_HMc/shimadzu/results_TOC/block2_18columns/", .), 
                              row.names = NULL, skip = 14, header = FALSE, col.names = eighteencolumns), .id = "source") 
# check structure
# str(shimadzu_2)


# combine the two blocks into one shimadzu results dataframe
shimadzu_results <- bind_rows(shimadzu_1, shimadzu_2)
# str(shimadzu_results)


# ---
## use function 'NumberXtract()'
# pull out numbers from alphanumerics to create:
# factor variable for trip numbers   (from 'source')
# numeric variable for NPOC-results  (from 'Result')
# factor variable for vial number    (from 'Sample.Name')
shimadzu_results <- shimadzu_results %>% 
  mutate(trip = as_factor(NumberXtract(source)),
         NPOC_mgL = as.numeric(NumberXtract(Result)),
         vial = as.numeric(NumberXtract(Sample.Name))) 

# check it out
# str(shimadzu_results)



# 3. ------------ SHIMADZU ANALYSIS RESULTS ------------ # 

# combine 'shimadzu_samples' with 'shimadzu_results'
TOCV_results <- shimadzu_results %>% 
  select(trip, vial, NPOC_mgL) %>% 
  group_by(trip, vial) %>% 
  summarize(NPOC_ppm = mean(NPOC_mgL)) %>% 
  right_join(shimadzu_samples, by = c("trip", "vial")) %>% 
  ungroup() 


# check head
head(TOCV_results)
# check structure
str(TOCV_results)
# check lengths match
nrow(TOCV_results) == nrow(shimadzu_samples)


# what about trip 21, which was actually in trip 20 (Cragg on 2019-12-19)
TOCV_results %>% filter(trip == "20" | trip == "21", site =="Cragg-crk") 

# update vials 19-21 to be "trip 21", for later matching with Odyssey stage data
TOCV_results <- TOCV_results %>% 
  mutate(#trip = as.character(trip),
    trip = case_when(trip == "20" & site =="Cragg-crk" & (vial == 19 | vial == 20 | vial == 21) ~ "21",
                     TRUE ~ trip),
    trip = as_factor(trip))
TOCV_results$NPOC_ppm = as.numeric(TOCV_results$NPOC_ppm)
# check to make sure it's updated:
TOCV_results %>% filter(trip == "20" | trip == "21", site =="Cragg-crk") 

# good!

```

## Spectrolyser 

-- proxy analysis results (CDOM equivalent DOC & TOC, NO~3~^-^, SAC~254~, SAC~436~)

The Spectrolyser is a full scan UV-VIS spectrophotometer. It uses a stable global calibration file to calculate equivalent concentrations of DOC, TOC, nitrate-nitrogen, and turbidity.

### Concentrations by proxy (UV-VIS)

The .par files generated by the spectrolyser contain results of equivalent concentrations based on spectral analysis and the internal global calibration file. A caveat to interpretting these results is that in order for NOM to be detected by UV-Vis absorption the molecules must absorb UV or Visible light (not all molecules do). UV-Vis absorption occurs only if the applied energy (light) can be absorbed by the molecule; in general, this required the presence of aromatic bonds (conjugated pi-bond systems in the molecule, a chromophore). Therefore, UV-Vis absorption is proportional to the molecule's degree of aromaticity.    

```{r, Spectrolyser-par}

## Spectrolyser samples and results
# sample IDs and results from analysis of optical properties
# multi-part data munge
# 1. bring in sample tracking data, format appropriately
# 2. bring in results data, format appropriately
# 3. bring them together as meanigful results data
# 4. some analyses were run in triplicate -- summarize as sample means


# 1. ------------ SAMPLES ------------ #

# create a data frame of all sample ID data
# for all samples analyzed on Scan Spectrolyser (UBC ESB 3062)


# read in all the files
spectrolyser_samples <- 
  list.files(path = "R-inputs_UBC-forWater-MSc_HMc/spectrolyser/samples_scan/", 
             pattern = "*.csv") %>% 
  purrr::map(~ read_csv(file.path("R-inputs_UBC-forWater-MSc_HMc/spectrolyser/samples_scan/", .), 
                        skip = 4, col_names = TRUE)) %>% 
  reduce(rbind)

# check structure
str(spectrolyser_samples)

# correct the structure/format
spectrolyser_samples <- spectrolyser_samples %>% 
  dplyr::transmute(measurement = as.numeric(measurement),
                   site = forcats::as_factor(site),
                   sample_type = forcats::as_factor(`sample-type`),
                   sample = forcats::as_factor(sample),
                   dt_sampled = head(lubridate::ymd_hms(`date-time_sampled`, 
                                                        frac = TRUE, tz = TZ, 
                                                        truncated = 3), -1),
                   fillStage_cm = as.numeric(`fill-stage`),
                   trip = as_factor(trip))
# check it
# str(spectrolyser_samples)
# head(spectrolyser_samples)

#check which trips were included
levels(spectrolyser_samples$trip)

# check which sites are included
levels(spectrolyser_samples$site)

# fix naming errors for sites
spectrolyser_samples$site <- spectrolyser_samples$site %>% 
  plyr::revalue(c(
    "Chris" = "Chris-crk",
    "Lower-Leech-blw-confl" = "Leech-downstreamconf",
    "Leech-main-confl" = "Leech-downstreamconf",
    "Leech-downstreamconf" = "Leech-downstreamconf",
    "J-Trib" = "West-Jordan",
    "West-jordan" = "West-Jordan",
    "W.Jarvis" = "Jarvis",
    "Judge" = "Judge-crk"
  ))
# check sites again
levels(spectrolyser_samples$site)

length(levels(spectrolyser_samples$site))
#---- samples were collected from a total of 31 sites (including lab), 6 were permanent field installations ---- #


# 2. ------------ RESULTS ------------ # 

# assign column names = col.names():
scancolumns <- c("Date", "Time",	"Status",	"Turbid.FTUeq",	"NULLTurbid",	"NO3-Neq_ppm",	"NULLNO3-Neq",	"TOCeq_ppm",	"NULLTOCeq", "DOCeq_ppm", "NULLDOCeq", "SAC254_Abs/m", "SAC254_0", "SAC436_Abs/m",	"SAC436_0",	"254-436_Abs/m", "254-436_0",	"analogIN_", "analogIN_0")

## read in files and add a column indicating trip #, based on source file name
spectrolyser_results <- 
  list.files(path = "R-inputs_UBC-forWater-MSc_HMc/spectrolyser/results_scan/") %>% 
  set_names(str_extract(., "([A-Za-z]{3,}+[0-9]*+)")) %>%
  purrr::map_dfr(~ read.table(
    file = file.path("R-inputs_UBC-forWater-MSc_HMc/spectrolyser/results_scan/", .),
    skip = 2, header = FALSE, col.names = scancolumns), .id = "source") 

# check it out
# str(spectrolyser_results)

# pull out numbers from alphanumeric 'source' to create factor variable 'trip'
# use function 'NumberXtract()'
# add measurement numbers to each trip group (analysis order)
# drop null variables for tidiness
spectrolyser_results <- spectrolyser_results %>% 
  mutate(trip = NumberXtract(source)) %>% 
  group_by(trip) %>% 
  mutate(measurement = row_number()) %>% 
  ungroup()

# check it out
#str(spectrolyser_results)

unique(spectrolyser_results$trip)

nrow(spectrolyser_results) == nrow(spectrolyser_samples)
# oh?! they should be the same length...
# several triplicate measurements -- see section 4 for solution


# 3. ------------ SPECTROLYSER ANALYSIS RESULTS ------------ # 

# combine 'spectrolyser_samples' with 'spectrolyser_results'
SCAN_results <- spectrolyser_results %>% 
  select(-c(Date, Time, Status, NULLTurbid, NULLNO3.Neq, NULLTOCeq, NULLDOCeq, SAC254_0, SAC436_0, X254.436_Abs.m, X254.436_0, analogIN_, analogIN_0)) %>% 
  full_join(spectrolyser_samples, by = c("trip", "measurement")) %>% 
  ungroup()

# check lengths match
nrow(SCAN_results) == nrow(spectrolyser_samples)
# check head
head(SCAN_results)
# check structure
#str(SCAN_results)


# what about trip 21, which was actually in trip 20 (Cragg on 2019-12-19)
SCAN_results %>% filter(trip == "20" | trip == "21", site =="Cragg-crk") 

# update vials 19-21 to be "trip 21", for later matching with Odyssey stage data
# also change trip 24 to 23 
## the samples were analyzed in different files 
## it seemed easier to mis-label them rather than manually altering the data files
SCAN_results <- SCAN_results %>% 
  mutate(trip = case_when(trip == "20" & site =="Cragg-crk" & (measurement == 19 | measurement == 20 | measurement == 21) ~ "21",TRUE ~ trip),
         trip = as_factor(trip))

# check to make sure it's updated:
SCAN_results %>% filter(trip == "20" | trip == "21", site =="Cragg-crk") 
# good!

# 4. -------------  triplicates!  ------------- 
# several analyses measured the same sample in triplicate to assess instrument precision
# trips c(10, 11, 12, 13, 15, 16)
# these measurements are counted as unique samples (though they are not)
# create an average for those measured in triplicate 
# split / apply / combine
SCAN_results <- SCAN_results %>%  
  group_by(trip, site, sample_type, sample, dt_sampled, fillStage_cm) %>% 
  summarise(Turbid.FTUeq = mean(Turbid.FTUeq),
            NO3.Neq_ppm = mean(NO3.Neq_ppm),
            TOCeq_ppm = mean(TOCeq_ppm), 
            DOCeq_ppm = mean(DOCeq_ppm),
            SAC254_Abs.m = mean(SAC254_Abs.m),
            SAC436_Abs.m = mean(SAC436_Abs.m)) %>% 
  ungroup()

```

### Full scan spectrophotometry (SCAN_results, spectral fingerprints)

The Spectrolyser outputs a fingerprint file containing absorbance values at all of the wavelengths monitored. These .fp files can provide information about NOM structure. For example, the ratio of the slope between 275-295nm and the slope from 350-400nm ("slope ratio (SR)"), commonly used as an indicator of molecular weight.

```{r Spectrolyser-fp, message=FALSE, warning=FALSE}

## Spectrolyser samples and fingerprints (fp)
# sample IDs and fullscans 
# multi-part data munge
# 1. bring in sample tracking data, format appropriately
# 2. bring in results data, format appropriately
# 3. bring them together as meanigful results data
# 4. some analyses were run in triplicate -- summarize as sample means


# 1. ------------ SAMPLES ------------ #
# these data have already been loaded
# "spectrtolyser_samples"

# 2. ------------ FULLSCAN RESULTS ------------ # 

# assign column names
fullscan_colnames <- c("Date", "Time", "Status", paste0("Abs_", seq(200.0, 750.0, by = 2.5)))
## read in files and add a column indicating trip #, based on source file name
spectrolyser_fullscan <- 
  list.files(path = "R-inputs_UBC-forWater-MSc_HMc/spectrolyser/fingerprints_scan_fp/") %>% 
  set_names(str_extract(., "([A-Za-z]{3,}+[0-9]*+)")) %>%
  purrr::map_dfr(~ read.table(
    file = file.path("R-inputs_UBC-forWater-MSc_HMc/spectrolyser/fingerprints_scan_fp/", .), 
    skip = 2, header = FALSE, col.names = fullscan_colnames), .id = "source") 

# check it out
# str(spectrolyser_fullscan)

# pull out numbers from alphanumeric 'source' to create factor variable 'trip'
# use function 'NumberXtract()'
# add measurement numbers to each trip group (analysis order)
# drop null variables for tidiness
spectrolyser_fullscan <- spectrolyser_fullscan %>% 
  mutate(trip = NumberXtract(source)) %>% 
  group_by(trip) %>%
  mutate(measurement = row_number()) %>% 
  ungroup() 

# note that trip "0" was an external calibration

# check it out
# str(spectrolyser_fullscan)
levels(spectrolyser_fullscan$trip)

colnames(spectrolyser_fullscan)

# str(spectrolyser_fullscan)
# str(spectrolyser_samples)
# combine?
nrow(spectrolyser_fullscan) == nrow(spectrolyser_samples)
# will have to handle triplicates as for .par files


# 3. ------------ SPECTROLYSER FULLSCAN RESULTS ------------ # 
# combine 'spectrolyser_samples' with 'spectrolyser_fullscan'
FULLSCAN_results <- full_join(spectrolyser_fullscan, spectrolyser_samples, 
                              by = c("trip", "measurement")) %>% 
  mutate(site = factor(site),
         sample_type = factor(sample_type),
         dt_sampled = lubridate::as_datetime(dt_sampled)
  )
# check columnnames
# colnames(FULLSCAN_results)
# levels(FULLSCAN_results$site)  


# 4. -------------- fixes:

# for Cragg Creek, trip 21 was actually in trip 20 (Cragg on 2019-12-19)
# update measurements c(19:21) to be "trip 21", for later matching with Odyssey stage data
FULLSCAN_results <- FULLSCAN_results %>% 
  mutate(trip = as.character(trip),
         trip = case_when(trip == "20" & site =="Cragg-crk" & 
                            (measurement == 19 | measurement == 20 | measurement == 21) ~ "21",
                          TRUE ~ trip))


# ---  triplicates:
# several analyses measured the same sample in triplicate to assess instrument precision
# trips c(10, 11, 12, 13, 15, 16)
# these measurements are counted as unique samples (though they are not)
# create an average for those measured in triplicate 
# split / apply / combine
FULLSCAN_results <- FULLSCAN_results %>% 
  select(-c("source", "Date", "Time", "Status")) %>% 
  group_by(trip, site, sample_type, sample, dt_sampled, fillStage_cm) %>% 
  summarise_all(list(~mean(.))) %>% 
  ungroup()
```

### Spectral indices
```{r, spectral-indices}

# ------ spectral indices -------
# the absorbance values can be used to determine spectral indices
# indices are more useful for interpretation than raw ABS values

# first, check that SAC254_Abs/m is ABS/m @254nm
spectral_results <- dplyr::full_join(FULLSCAN_results, SCAN_results,
                                     by = c("trip", "site", "sample_type", "sample", "dt_sampled", "fillStage_cm"))

# explore 
spectral_results %>%
  group_by(site, sample_type, sample, SAC254_Abs.m) %>% 
  summarise(twofiftyfour = mean(c(Abs_252.5, Abs_255)))  # actually abs at 253.75
# I believe it.

# how was turbidity?
spectral_results %>%
  filter(Turbid.FTUeq > 0) %>% 
  select(trip, site, sample_type, sample, SAC254_Abs.m, Abs_252.5, Abs_255, Turbid.FTUeq)

# how many samples will I lose by filtering out those with turbidity>0?
turby <- nrow(spectral_results %>%
                filter(Turbid.FTUeq > 0.000) %>% 
                select(trip, site, sample_type, sample, SAC254_Abs.m, Turbid.FTUeq)
)
loss <- 100*(turby/nrow(spectral_results))

# -------------------------------------------------------------------------------------------
# Spectral slopes -- from linear regression of log~e~-transformed spectra

# S1
# 275-295 nm (S~275-295~) == S1
S1 <- spectral_results %>%
  filter(Turbid.FTUeq == 0) %>%                   # remove turbid samples 
  pivot_longer(cols = Abs_275:Abs_295,            # longer is better (select only what you need)
               names_to = "wavelength_nm", values_to = "SAC_per.m") %>% 
  select(c(trip, site, sample_type, sample, dt_sampled, fillStage_cm, 
           Turbid.FTUeq, NO3.Neq_ppm, TOCeq_ppm, DOCeq_ppm, 
           SAC254_Abs.m, SAC436_Abs.m, wavelength_nm, SAC_per.m)
  ) %>% 
  mutate(wavelength_nm = as.numeric(NumberXtract(wavelength_nm))) %>%  # numeric wavelength
  mutate(lnSAC = log(SAC_per.m)) %>% 
  group_by(trip, site, sample_type, sample) %>% 
  summarise(sample_slope1 = cor(y = lnSAC, x = wavelength_nm)) 

# plot it
spectral_results %>%
  filter(Turbid.FTUeq == 0) %>% 
  pivot_longer(cols = Abs_275:Abs_295,            # longer is better (select only what you need)
               names_to = "wavelength_nm", values_to = "SAC_per.m") %>% 
  mutate(wavelength_nm = as.numeric(NumberXtract(wavelength_nm)),  # numeric wavelength
         lnSAC = log(SAC_per.m)) %>%  
  group_by(trip, site, sample_type, sample) %>% 
  ggplot(aes(x = wavelength_nm, y = lnSAC)) +
  geom_jitter() +
  geom_smooth(method = "lm")

# S2
# 350-400 nm (S~350-400~) == S2
S2 <- spectral_results %>%
  filter(Turbid.FTUeq == 0) %>%                   # remove turbid samples 
  pivot_longer(cols = Abs_350:Abs_400,            # longer is better (select only what you need)
               names_to = "wavelength_nm", values_to = "SAC_per.m") %>% 
  select(c(trip, site, sample_type, sample, dt_sampled, fillStage_cm, 
           Turbid.FTUeq, NO3.Neq_ppm, TOCeq_ppm, DOCeq_ppm, 
           SAC254_Abs.m, SAC436_Abs.m, wavelength_nm, SAC_per.m)
  ) %>% 
  mutate(wavelength_nm = as.numeric(NumberXtract(wavelength_nm))) %>%  # numeric wavelength
  mutate(lnSAC = log(SAC_per.m)) %>% 
  group_by(trip, site, sample_type, sample) %>% 
  summarise(sample_slope2 = cor(y = lnSAC, x = wavelength_nm)) 

# plot it
spectral_results %>%
  filter(Turbid.FTUeq == 0) %>% 
  pivot_longer(cols = Abs_350:Abs_400,            # longer is better (select only what you need)
               names_to = "wavelength_nm", values_to = "SAC_per.m") %>% 
  mutate(wavelength_nm = as.numeric(NumberXtract(wavelength_nm)),  # numeric wavelength
         lnSAC = log(SAC_per.m)) %>%  
  group_by(trip, site, sample_type, sample) %>% 
  ggplot(aes(x = wavelength_nm, y = lnSAC)) +
  geom_jitter() +
  geom_smooth(method = "lm")

# Summary df with slope ratios and spectral indicies
spectral_summary <- spectral_results %>%
  mutate(pseudo254 = (Abs_252.5+Abs_255)/2) %>% 
  select(trip:fillStage_cm,
         pseudo254,
         Abs_250, Abs_365
  ) %>% 
  full_join(S1) %>% 
  full_join(S2) %>% 
  mutate(SlopeRatio = sample_slope1/sample_slope2,
         E2E3 = Abs_250/Abs_365,
         trip = as_factor(trip)) %>% 
  select(-c(sample_slope1, sample_slope2))

```

## Data collation

* Join results files from Shimadzu and Spectrolyser (direct and indirect measures of DOC, plus full scan data)
* Bringing together the results from both the Shimadzu and Spectrolyser. 
* Join data frames and create additional variables to help classify the data for plotting. 

```{r, compile-analyses-results}

# join the results files from the Shimadzu TOC-V and Spectrolyser
#str(TOCV_results)   # shimadzu NPOC results (direct measure of DOC)
#str(SCAN_results)   # spectrolyser indirect results of DOC (UV-Vis)
#str(spectral_summary)  # spectrolyser full scan 250-700nm

# both have c("trip", "site", "sample_type", "sample", "dt_sampled", "fillStage_cm")
# the variables measured are: NPOC_ppm, Turbid.FTUeq, NO3.Neq_ppm, TOCeq_ppm, DOCeq_ppm, SAC254_Abs.m, SAC436_Abs.m
# the instrument was TOCV or SCAN

# --- WIDE SUMMARY DF --- # 
sampleresults <- full_join(TOCV_results, SCAN_results, 
                           by = c("trip", "site", "sample_type", "sample", "dt_sampled", "fillStage_cm")) %>% 
  full_join(spectral_summary, by = c("trip", "site", "sample_type", "sample", "dt_sampled", "fillStage_cm")) %>% 
  mutate(trip = as_factor(trip),
         site = as_factor(site),
         sample_type = as_factor(sample_type),
         sample = as_factor(sample),
         analysis = factor(analysis, exclude = "TOC"),  # drop "TOC"
         analysis = factor(analysis),   # drop residual NA from removing "TOC"
         site = factor(site, exclude = "Mystery-bottle"),  # drop "mystery bottle"
         site = factor(site)) %>%  # drop residual NA from removing "mystery bottle"
  select(-c(vial))   # drop unnecessary variables
# check it out
#str(sampleresults)

# add a column of "trip_end" to have a date for each trip (plotting)
## this is a terrible method
## there must be a way to use a loop/map/apply (don't know how yet)

#sampleresults2 <-  sampleresults %>% 
#   mutate(
#    for (i in seq_along(1:22)) {
#      trip_end[i] = case_when(trip == i ~ trip_df$trip.end[i])
#        })

sampleresults <- sampleresults %>% 
  mutate(trip_end = case_when(  
    trip == "1" ~ trip_df$trip.end[1],
    trip == "2" ~ trip_df$trip.end[2],
    trip == "3" ~ trip_df$trip.end[3],
    trip == "4" ~ trip_df$trip.end[4],
    trip == "5" ~ trip_df$trip.end[5],
    trip == "6" ~ trip_df$trip.end[6],
    trip == "7" ~ trip_df$trip.end[7],
    trip == "8" ~ trip_df$trip.end[8],
    trip == "9" ~ trip_df$trip.end[9],
    trip == "10" ~ trip_df$trip.end[10],
    trip == "11" ~ trip_df$trip.end[11],
    trip == "12" ~ trip_df$trip.end[12],
    trip == "13" ~ trip_df$trip.end[13],
    trip == "14" ~ trip_df$trip.end[15],
    trip == "15" ~ trip_df$trip.end[16],
    trip == "16" ~ trip_df$trip.end[17],
    trip == "17" ~ trip_df$trip.end[18],
    trip == "18" ~ trip_df$trip.end[19],
    trip == "19" ~ trip_df$trip.end[20],
    trip == "20" ~ trip_df$trip.end[21],
    trip == "21" ~ trip_df$trip.end[22],
    trip == "22" ~ trip_df$trip.end[23],
    trip == "23" ~ trip_df$trip.end[24])) %>% 
  mutate(trip_end = lubridate::ymd(trip_end))

# add a variable to identify season 
# three seasons (early/late wet and dry) 
# and two seasons (wet/dry))    
sampleresults <- sampleresults %>% 
  mutate(trip = factor(trip, levels = 0:23),
         three_seasons = case_when(
           trip == 0 ~ "SCAN QA-QC", 
           trip %in% 1:4 ~ "early wet [Oct-Dec]",   # (second half) Oct-Dec 2018
           trip %in% 5:9 ~ "late wet [Jan-May]",           # Jan-May 2019 
           trip %in% 10:16 ~ "summer [June-Oct]",  # June-Oct (first half) 2019
           trip %in% 17:21 ~ "early wet [Oct-Dec]", # Oct-Dec 2019
           trip %in% 22:23 ~ "late wet [Jan-May]"), # Jan-Feb 2020
         two_seasons = case_when(
           trip == 0 ~ "SCAN QA-QC", 
           trip %in% 1:9 ~ "wet",   # mid-Oct 2018 to May 2019
           trip %in% 10:16 ~ "dry",  # June to mid-Oct 2019
           trip %in% 17:23 ~ "wet")) # mid-Oct 2019 to Jan 2020

# SUVA ------ 
# add a variable for SUVA = [DOC]/SAC254
sampleresults  <-  sampleresults %>% 
  mutate(SUVA = SAC254_Abs.m/NPOC_ppm)

# good!

```

## Metals data
```{r, CRD-metals}

# I collected metals samples from trip 3 to 10
# CRD provided results files (PDF report) for trips 3 to 8
# data was manually sorted from PDF (there must be a better way)
# load data
metals <- read_csv("R-inputs_UBC-forWater-MSc_HMc/Metals_CRD-forWaterMSc_HMc/CRD-metals-data_collated-trips3-10_nocharacternulls.csv", 
                   col_names = TRUE) %>% 
  tidyr::pivot_longer(cols = WESTLEECH:CHRISCREEK,
                      names_to = "site",
                      values_to = "metals_values") %>% 
  mutate(Trip = factor(Trip),
         Parameters = factor(Parameters),
         site = factor(site))


# check site names
levels(metals$site)

# add data for NPOC, DOC_eq, and SUVA
# subset for the six installation sites & rename
metalslab <- left_join(x = metals, 
                       y = (sampleresults %>% 
                              filter(site == "Weeks-out" |
                                       site == "Leech-head" |
                                       site == "Chris-crk" |
                                       site == "Cragg-crk" |
                                       site == "West-Leech"| 
                                       site == "Tunnel",
                                     sample_type == "Grab" & sample == "Grab",
                                     analysis == "DOC") %>% 
                              mutate(site = fct_recode(site, 
                                                       WEEKSOUT = "Weeks-out", 
                                                       CHRISCREEK = "Chris-crk", 
                                                       LEECHHEAD = "Leech-head", 
                                                       CRAGGCREEK = "Cragg-crk", 
                                                       WESTLEECH = "West-Leech", 
                                                       TUNNEL = "Tunnel"))),
                       by = c("site" = "site", "Trip" = "trip")) %>%
  mutate(Trip = factor(Trip),
         Site = factor(site),
         metal_parameters = factor(Parameters)) %>% 
  drop_na(site, metal_parameters) %>% 
  select(-"BONEYARD") %>% 
  mutate()

```


# Field Data

The six pirmary research location in the Leech River watershed were equipped with vertical sampling racks (which combined passive (siphon) samplers and compact river stage loggers) colected continuous water level via Odyssey capacitiance water level loggers. Field data includes the following: 

1. The CRD provided data from their fire weather stations for precipitation and air temperature. 

2. At the six installation sites, I collected data for:

* water level (Odyssey capacitance water level loggers)
* air and water temperature (Hobo TidbiTs)

3. At the four mainstem sites (Leech Head, Cragg Creek, West Leech, Tunnel), I collected 10 minute interval triggered trail-cam photos.

## Weather data (CRD fire weather stations)

The Leech river watershed hydroclimatic regime is pluvial, therefore precipitation data is very important. There were (at the time of my research) three weather stations in proximity to the Leech:

* Chris Crk WxStn at the headwaters
* Survey Mtn WxStn at the highest peak, in the Leech (installed in 2019, short record)
* Martin's Gulch WxStn near the Leech River Tunnel (future point of diversion)

## Weather
```{r, Wx-precip}

# bring in WxStn data 
# Note: each file has a different DateTime format (be sure to lubridate)

# read it in
precip_data <- 
  list.files("R-inputs_UBC-forWater-MSc_HMc/CRD_FWx-Data/", pattern = "*.CSV") %>% 
  #set_names(str_extract(., "([A-ZA-Z]{4,}+)")) %>%
  purrr::map_dfr(~ read_csv(file.path("R-inputs_UBC-forWater-MSc_HMc/CRD_FWx-Data/", .), 
                            col_names = TRUE,
                            col_types = list("c", "c", "d", "d", "d", "d", "d", "d", "d", "d", "d", "d"))) %>% 
  mutate(DateTime = lubridate::parse_date_time(DateTime, c("dmY HM", "Ymd HM", "Ymd HMS", "Ymd HMS Op!*")))


# create a summary datafame with mean weather data between Chris creek and Martin's Gulch
wx_mean <- precip_data %>% 
  filter(StationName == "FWx Chris Creek" | StationName == "FWx Martins Gulch") %>% 
  group_by(DateTime) %>% 
  select(-StationName) %>% 
  summarise_all(list(mean = mean))

```


## River stage data 

At the six installation sites, water level loggers were installed (Odyssey capacitance water level loggers). The level loggers recorded stage at 10 minute intervals. Each logger was in a stilling well with a secured external stage measuring tape; the stage data must be adjusted with an offset value to match with the observed stage in order to match sample collection to logger datetimes.


```{r, stage}

# bring in logger data and add a id column from the site file name
# note, each of the 6 sites has files named except 'Leech-Head' is named by the logger serial number (12040) 

# input samples directory path
odyssey_path <-  "R-inputs_UBC-forWater-MSc_HMc/odyssey/"

# check files  
list.files(path = odyssey_path, pattern = "*.CSV") 
# check source extract is reasonable   
list.files(path = odyssey_path, pattern = "*.CSV") %>%
  str_extract("([A-Z0-9]{4,}+)")
# Note that source names will have to be updated (mutate)

# read in all the files as one dataframe
# add source (ID) based on file names
# adjust date-time (note that there were multiple date formats output by loggers)
# time is in 24 hour clock and it seems that lubridate does not like that...
odyssey_data <- list.files(path = odyssey_path, pattern = "*.CSV") %>% 
  set_names(str_extract(., "([A-Z0-9]{4,}+)")) %>%
  purrr::map_dfr(~ read_csv(file.path(odyssey_path, .), skip = 12, 
                            col_names = c("scan_no.", "Date", "Time", "Capacitance", "stage_cm")), .id = "source") %>% 
  mutate(source = forcats::as_factor(source),
         Date = lubridate::parse_date_time(Date, c("dmy", "ydm"), tz = TZ),
         DateTime = lubridate::ymd_hms(paste(Date, Time), tz = TZ))

# check structure
# str(odyssey_data)

# check levels
levels(odyssey_data$source)
# rename site IDs
odyssey_data$source <- forcats::fct_recode(odyssey_data$source, 
                                           LeechHead = "12040",
                                           ChrisCrk = "CHRIS",
                                           CraggCrk = "CRAGG",
                                           Tunnel = "LEECH",
                                           Weeks = "WEEKS",
                                           WestLeech = "WEST")
# re-check levels
levels(odyssey_data$source)


# there is certainly a better way to do this -- with a loop or map or apply or metaprogramming
# I don't know yet and I need to move on. 
# bad technique: copy and paste x22 (I know this is bad and ugly, sorry)
# ----
odyssey_data <- odyssey_data %>%
  select(Date, source, stage_cm, DateTime) %>%
  mutate(interval = case_when(  
    Date %within% interval(trip_df$trip.start[1], trip_df$trip.end[2]) ~ trip_df$trip[2],
    Date %within% interval(trip_df$trip.end[2], trip_df$trip.end[3]) ~ trip_df$trip[3],
    Date %within% interval(trip_df$trip.end[3], trip_df$trip.end[4]) ~ trip_df$trip[4],
    Date %within% interval(trip_df$trip.end[4], trip_df$trip.end[5]) ~ trip_df$trip[5],
    Date %within% interval(trip_df$trip.end[5], trip_df$trip.end[6]) ~ trip_df$trip[6],
    Date %within% interval(trip_df$trip.end[6], trip_df$trip.end[7]) ~ trip_df$trip[7],
    Date %within% interval(trip_df$trip.end[7], trip_df$trip.end[8]) ~ trip_df$trip[8],
    Date %within% interval(trip_df$trip.end[8], trip_df$trip.end[9]) ~ trip_df$trip[9],
    Date %within% interval(trip_df$trip.end[9], trip_df$trip.end[10]) ~ trip_df$trip[10],
    Date %within% interval(trip_df$trip.end[10], trip_df$trip.end[11]) ~ trip_df$trip[11],
    Date %within% interval(trip_df$trip.end[11], trip_df$trip.end[12]) ~ trip_df$trip[12],
    Date %within% interval(trip_df$trip.end[12], trip_df$trip.end[13]) ~ trip_df$trip[13],
    Date %within% interval(trip_df$trip.end[13], trip_df$trip.end[15]) ~ trip_df$trip[15], # changed pattern
    Date %within% interval(trip_df$trip.end[15], trip_df$trip.end[16]) ~ trip_df$trip[16],
    Date %within% interval(trip_df$trip.end[16], trip_df$trip.end[17]) ~ trip_df$trip[17],
    Date %within% interval(trip_df$trip.end[17], trip_df$trip.end[18]) ~ trip_df$trip[18],
    Date %within% interval(trip_df$trip.end[18], trip_df$trip.end[19]) ~ trip_df$trip[19],
    Date %within% interval(trip_df$trip.end[19], trip_df$trip.end[20]) ~ trip_df$trip[20],
    Date %within% interval(trip_df$trip.end[20], trip_df$trip.end[21]) ~ trip_df$trip[21],
    Date %within% interval(trip_df$trip.end[21], trip_df$trip.end[22]) ~ trip_df$trip[22],
    Date %within% interval(trip_df$trip.end[22], trip_df$trip.end[23]) ~ trip_df$trip[23],
    Date %within% interval(trip_df$trip.end[23], trip_df$trip.end[24]) ~ trip_df$trip[24]) )
# note that interval (odyssey_data) is the same as (trip_df) trip number of analysis/collection 


# NOTE --- These logger data need to be adjusted with the observed stage offset
# -------
# offsets based on data download from fall 2019
# each offset should be added to logged data to match rack stages
offset_L.Head   <- 95-38.907   # 2020-10-11: obs 95cm, recorded = 38.907 cm
offset_WksOut   <- 26.0-20.87  # 2020-09-20: obs = 26.0 cm, recorded = 20.87
offset_ChrisCrk <- 19-13.196   # 2020-09-20: obs = 19cm, recorded = 13.196 cm
offset_CraggCrk <- 8.0 - 6.57  # 2020-09-20: obs = 8.0 cm, recorded = 6.57 cm
offset_W.Leech  <- 15.5- 12.19 # 2020-09-20: obs = 15.5 cm, recorded = 12.188 cm
offset_Tunnel   <- 0.0 - 3.35  # 2020-09-20: obs = 0 cm (HFSG = 27cm), recorded = 3.35
# ---
# these are old values (2018)...in good agreement 
# offset_L.Head   <- 60.84 # 
# offset_WksOut   <- 5.17 # add to logger (was 29, 9.5)
# offset_ChrisCrk <- 5.75 # add to logger (was 30, 7.27)
# offset_CraggCrk <- 1.43 # add to logger 19-09-20 (was 9.25)
# offset_W.Leech  <- 2.87 # add to logger (was 4.85) 
# offset_Tunnel   <- -3.32 # add to logger (negative offset, weird, was -3.17)
# ---

# corrected_stage
corr_stage <- odyssey_data %>% 
  group_by(source) %>% 
  mutate(corr_stage_cm = case_when(
    source == "LeechHead" ~ (stage_cm + offset_L.Head),
    source == "ChrisCrk" ~ (stage_cm + offset_ChrisCrk),  
    source == "CraggCrk" ~ (stage_cm + offset_CraggCrk),  
    source == "Tunnel" ~ (stage_cm + offset_Tunnel),    
    source == "Weeks" ~ (stage_cm + offset_WksOut),     
    source == "WestLeech" ~ (stage_cm + offset_W.Leech)
  )) %>% 
  ungroup() %>% 
  mutate(source = factor(source, 
                         levels = c("Weeks", "ChrisCrk", "LeechHead", "CraggCrk", "WestLeech", "Tunnel")))

```



# WIP Events defined based on precip using Rainmaker::RMevents
```{r, warnings = FASLE, messages = FALSE}
# use precipitation to define events
# precip_data
# odyssey_data

Wx_CC_storms <- wx_mean %>% 
  filter(DateTime >= "2018-10-23") %>% 
  as.data.frame() %>% 
  Rainmaker::RMevents(., ieHr = 20, rainthresh = 30, rain = "Rn15_mean", time = "DateTime")

Wx_CC_storms[["storms2"]]  # list of events that surpass rainthresh (mm event depth)
Wx_CC_storms[["storms"]]   # list of all events


# rain plot with storm lines
rn_strm_plot <- subbasin_meanrain_plot <- wx_mean %>% 
  mutate(date = lubridate::as_date(DateTime),
         Rain = factor("Rain")) %>% 
  group_by(date, Rain) %>% 
  dplyr::summarise(daily_rn = sum(Rn_1_mean, na.rm = TRUE)) %>% 
  ungroup()  %>% 
  filter(date >= "2018-10-24") %>% 
  ggplot(aes(x = date, y = daily_rn)) +
  geom_col(aes(colour = Rain), colour = "#0072B2") +
  scale_y_reverse() +
  labs(x = "", y = "mm /day") +
  theme_bw() +
  geom_vline(xintercept = as_date(Wx_CC_storms[["storms2"]]$StartDate))+
  #scale_x_date(date_breaks = "1.5 months", date_labels = "%Y-%m")+ # use to check alignment 
  scale_x_date(date_breaks = "1.5 months", labels = NULL)+  # remove axis labels when you're confident
  facet_wrap(~Rain, ncol = 1, 
             strip.position = "right") +
  theme(legend.position = "none",
        strip.background = element_blank(),
        axis.text.x = element_text(angle = 90))

# river stage with storm lines
rv_strm_plot <- stage_plot+
  geom_vline(xintercept = Wx_CC_storms[["storms2"]]$StartDate)

# stack the two storm delineated plots
cowplot::plot_grid(rn_strm_plot, rv_strm_plot, 
                   ncol = 1, axis = "l", align = "v",
                   rel_heights = c(1,5))



```


# Match bottle height to logged stage

Instead of trying to extract DateTime from odyssey data to add to corresponding stage in the sample df, what if you join them by site and trip, but first group stage by unique values such that only the first occurrence is used to join?

```{r, join-stage-samples}

str(corr_stage)
str(sampleresults)

# remove dashes from sub-basin names (match Odyssey names)
sampleresults <- sampleresults %>% 
  mutate(site = forcats::fct_recode(site, 
                                    Weeks = "Weeks-out",
                                    ChrisCrk = "Chris-crk",
                                    LeechHead = "Leech-head",
                                    CraggCrk = "Cragg-crk", 
                                    WestLeech = "West-Leech"))

# isolate subbasins and rename 
sample_df <- sampleresults %>% 
  select(c(site, trip, sample, fillStage_cm, sample_type, analysis, NPOC_ppm)) %>% 
  rename(rackstage_cm = fillStage_cm) %>% 
  mutate(rackstage_cm = round(rackstage_cm)) %>% 
  filter(site == "Weeks" |
         site == "ChrisCrk" | 
         site == "LeechHead" | 
         site == "CraggCrk" | 
         site == "WestLeech" | 
         site == "Tunnel",
         sample_type == "Rack",
         analysis == "DOC",
         trip != 0) %>% 
  droplevels() %>% 
  group_by(site, trip)

# this is corrected stage used for mapping samples to DateTime
stage_df <- corr_stage %>% 
  group_by(source, interval) %>% 
  select(-c(Date, stage_cm)) %>% 
  rename(rackstage_cm = corr_stage_cm,
         site = source,
         trip = interval) %>% 
  mutate(rackstage_cm = round(rackstage_cm)) %>% 
  dplyr::distinct(rackstage_cm, .keep_all = TRUE)

# Join stage and sample dataframes
# then, drop all the NA values so you just have sampleresults with 
sampleresults2 <- left_join(stage_df, sample_df, 
                 by = c("site", "trip", "rackstage_cm")) %>% 
  dplyr::distinct(NPOC_ppm, sample, .keep_all = TRUE)

# great! now, you have a Rack-sample dataframe with DateTime!!!
# next, you need to join that with the original sampleresults that have Grab and QA-QC results for a full dataset :)

wut <- left_join(sampleresults, sampleresults2, 
                 by = c("site", "trip", "sample_type", "sample", "analysis", "NPOC_ppm")) %>% 
  dplyr::filter(!is.na(NPOC_ppm) | !is.na(DOCeq_ppm))  
# there were some ghost rows full of NA (likely imported from Excel)

wutwut <- within(wut, 
                 DateTime_sampled <- ifelse(
                   is.na(dt_sampled), DateTime, dt_sampled)) %>% 
  mutate(DateTime_sampled = lubridate::as_datetime(DateTime_sampled, tz = TZ))
# also, make a new column for sample_stage
wutwutwuw <- within(wutwut, 
                 sample_stage <- ifelse(
                   is.na(rackstage_cm), fillStage_cm, rackstage_cm))
# great jorb!
# save as a new dataframe
results <- wutwut %>% 
  select(-c("dt_sampled", "trip_end", "three_seasons", "DateTime")) %>% 
  mutate(trip = factor(trip), #, levels = c(0:23)),
         site = factor(site), #levels = c("Weeks", "ChrisCrk", "LeechHead", "CraggCrk", "WestLeech", "Tunnel")),
         sample_type = factor(sample_type),
         analysis = factor(analysis),
         two_seasons = factor(two_seasons))

```


# File Outputs

save the compiled and formatted dataframes as .csv files.
Keep this as a code-note so to prevent over-writing files unintentionally

``` 
# {r output sample analysis files}

## save all compiled sample analysis as .csv files 
# keep as code note to avoid accidental over-writing later*

# TOCV_results df
# Shimadzu sample analyses results
write_csv(TOCV_results, path = "R-outputs_UBC-forWater-MSc_HMc/TOCV_results.csv", na = "NA")   

# SCAN_results df
# Spectrolyser sample analyses results
write_csv(SCAN_results, path = "R-outputs_UBC-forWater-MSc_HMc/SCAN-par_results.csv", na = "NA")  

# FULLSCAN_results df
# Spectrolyser fullscan (.fp) analyses results
write_csv(FULLSCAN_results, path = "R-outputs_UBC-forWater-MSc_HMc/SCAN-fp_results.csv", na = "NA")

# spectral_results df
# Spectrolyser fullscan (.fp) analyses results
write_csv(spectral_results, path = "R-outputs_UBC-forWater-MSc_HMc/spectral_results.csv", na = "NA")

# sampleresults df
# compiled sample analyses results (wide)
write_csv(sampleresults, 
path = "R-outputs_UBC-forWater-MSc_HMc/samples-lab-analyses_results.csv", na = "NA")  

# metalslab df
# metals sample analyses results with OC (long)
write_csv(metalslab, 
path = "R-outputs_UBC-forWater-MSc_HMc/metals-DOCgrab-sample_results-long.csv", na = "NA")  

# odyssey_data df
# stage data compiled with interval/trip
write_csv(odyssey_data, 
path = "R-outputs_UBC-forWater-MSc_HMc/Odyssey-stage_compiled.csv", na = "NA")

# corr_stage df
# baseflow and rack corrected Odyssey data
write_csv(corr_stage, 
path = "R-outputs_UBC-forWater-MSc_HMc/Odyssey-RackCorrected-stage.csv", na = "NA")

# precip_data df
# 2018-2020 weather station data compiled and formatted
write_csv(precip_data, 
path = "R-outputs_UBC-forWater-MSc_HMc/FWx-PrecipTemp_compiled.csv", na = "NA")

# wx_mean df
# 2018-2020 weather station data averages from FWx stations Chris Creek and Martins Gulch
write_csv(wx_mean, 
path = "R-outputs_UBC-forWater-MSc_HMc/FWx-Mean-LWSA_PrecipTemp.csv", na = "NA")

# results
# Complete sample results dataframe with sample DateTimes!
write_csv(results, 
path = "R-outputs_UBC-forWater-MSc_HMc/Results_complete.csv", na = "NA")

```


# fin

For some reason, RStudio truncates the session viewable... so here's some text to hold the place.
```{r}
# and an empty code chunk
```








